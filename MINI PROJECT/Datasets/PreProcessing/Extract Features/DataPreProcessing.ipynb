{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('malicious_phish.csv')\n",
    "# Extract the URLs from the 'url' column of the DataFrame\n",
    "urls = df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651191"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_url(url):\n",
    "    # check if url starts with http or https\n",
    "    if url.startswith('http://') or url.startswith('https://'):\n",
    "        return url\n",
    "    \n",
    "    # check if domain name is accessible with https\n",
    "    try:\n",
    "        response = requests.get(f'https://{url}', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return f'https://{url}'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # check if domain name is accessible with http\n",
    "    # try:\n",
    "    #     response = requests.get(f'http://{url}', timeout=3)\n",
    "    #     if response.status_code == 200:\n",
    "    #         return f'http://{url}'\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    # if all else fails, add http:// protocol and return\n",
    "    return f'http://{url}'\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    final_urls = []\n",
    "    for url in chunk:\n",
    "        final_url = get_final_url(url)\n",
    "        final_urls.append(final_url)\n",
    "    return final_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 URLs\n",
      "Processed 20000 URLs\n",
      "Processed 30000 URLs\n",
      "Processed 40000 URLs\n",
      "Processed 50000 URLs\n",
      "Processed 60000 URLs\n",
      "Processed 70000 URLs\n",
      "Processed 80000 URLs\n",
      "Processed 90000 URLs\n",
      "Processed 100000 URLs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv('malicious_phish.csv')\n",
    "    \n",
    "    # Extract the URLs from the 'url' column of the DataFrame\n",
    "    urls = df['url']\n",
    "    \n",
    "    # Chunk the URLs into smaller batches\n",
    "    chunk_size = 1000\n",
    "    url_chunks = [urls[i:i+chunk_size] for i in range(0, len(urls), chunk_size)]\n",
    "    \n",
    "    # Process the URLs using ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        # Submit each URL batch for processing\n",
    "        futures = [executor.submit(process_chunk, url_chunk) for url_chunk in url_chunks]\n",
    "        \n",
    "        # Print progress at every 10000 URLs\n",
    "        counter = 0\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            result = future.result()\n",
    "            df.loc[counter:counter+len(result)-1, 'final_url'] = result\n",
    "            counter += len(result)\n",
    "            if counter % 10000 == 0:\n",
    "                print(f'Processed {counter} URLs')\n",
    "            \n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv('output_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnish\\AppData\\Local\\Temp\\ipykernel_14860\\2445935755.py:47: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  url_chunks = [urls[i:i+chunk_size] for i in range(0, len(urls), chunk_size)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "def get_final_url(url):\n",
    "    # check if url starts with http or https\n",
    "    if url.startswith('http://') or url.startswith('https://'):\n",
    "        return url\n",
    "    \n",
    "    # check if domain name is accessible with https\n",
    "    try:\n",
    "        response = requests.get(f'https://{url}', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return f'https://{url}'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # check if domain name is accessible with http\n",
    "    # try:\n",
    "    #     response = requests.get(f'http://{url}', timeout=3)\n",
    "    #     if response.status_code == 200:\n",
    "    #         return f'http://{url}'\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    # if all else fails, add http:// protocol and return\n",
    "    return f'http://{url}'\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    final_urls = []\n",
    "    for url in chunk:\n",
    "        final_url = get_final_url(url)\n",
    "        final_urls.append(final_url)\n",
    "    return final_urls\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv('malicious_phish.csv')\n",
    "    \n",
    "    # Extract the URLs from the 'url' column of the DataFrame\n",
    "    start_index = 600001\n",
    "    # end_index = 600000\n",
    "    urls = df['url'][start_index:]\n",
    "    \n",
    "    # Chunk the URLs into smaller batches\n",
    "    chunk_size = 1000\n",
    "    url_chunks = [urls[i:i+chunk_size] for i in range(0, len(urls), chunk_size)]\n",
    "    \n",
    "    # Process the URLs using ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        # Submit each URL batch for processing\n",
    "        futures = [executor.submit(process_chunk, url_chunk) for url_chunk in url_chunks]\n",
    "        \n",
    "        # Print progress at every 10000 URLs\n",
    "        counter = start_index\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            result = future.result()\n",
    "            df.loc[counter:counter+len(result)-1, 'final_url'] = result\n",
    "            counter += len(result)\n",
    "            if counter % 10000 == 0:\n",
    "                print(f'Processed {counter} URLs')\n",
    "            \n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv('output_final 600000 to EOF.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "def get_final_url(url):\n",
    "    # check if url starts with http or https\n",
    "    if url.startswith('http://') or url.startswith('https://'):\n",
    "        return url\n",
    "    \n",
    "    # check if domain name is accessible with https\n",
    "    try:\n",
    "        response = requests.get(f'https://{url}', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return f'https://{url}'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # check if domain name is accessible with http\n",
    "    # try:\n",
    "    #     response = requests.get(f'http://{url}', timeout=3)\n",
    "    #     if response.status_code == 200:\n",
    "    #         return f'http://{url}'\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    # if all else fails, add http:// protocol and return\n",
    "    return f'http://{url}'\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    final_urls = []\n",
    "    for url in chunk:\n",
    "        final_url = get_final_url(url)\n",
    "        final_urls.append(final_url)\n",
    "    return final_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnish\\AppData\\Local\\Temp\\ipykernel_8224\\2072373625.py:12: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  url_chunks = [urls[i:i+chunk_size] for i in range(0, len(urls), chunk_size)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tnish\\Downloads\\MINI PROJECT\\DataPreProcessing.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m counter \u001b[39m=\u001b[39m start_index\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m temp_dfs \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, future \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(futures)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39mresult()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     temp_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mfinal_url\u001b[39m\u001b[39m'\u001b[39m: result})\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    241\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m    243\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 245\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[0;32m    248\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv('malicious_phish.csv')\n",
    "    \n",
    "    # Extract the URLs from the 'url' column of the DataFrame\n",
    "    start_index = 1\n",
    "    end_index = 1000\n",
    "    urls = df['url'][start_index:end_index]\n",
    "    \n",
    "    # Chunk the URLs into smaller batches\n",
    "    chunk_size = 1000\n",
    "    url_chunks = [urls[i:i+chunk_size] for i in range(0, len(urls), chunk_size)]\n",
    "    \n",
    "    # Process the URLs using ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        # Submit each URL batch for processing\n",
    "        futures = [executor.submit(process_chunk, url_chunk) for url_chunk in url_chunks]\n",
    "        \n",
    "        # Print progress at every 10000 URLs\n",
    "        counter = start_index\n",
    "        temp_dfs = []\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            result = future.result()\n",
    "            temp_df = pd.DataFrame({'final_url': result})\n",
    "            temp_dfs.append(temp_df)\n",
    "            counter += len(result)\n",
    "            if counter % 10000 == 0:\n",
    "                print(f'Processed {counter} URLs')\n",
    "            \n",
    "    # Concatenate the temporary DataFrames and update the original DataFrame\n",
    "    temp_df = pd.concat(temp_dfs, ignore_index=True)\n",
    "    df = df.iloc[start_index:].reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_df], axis=1)\n",
    "    \n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv('output_final 600000 to EOF.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL SCRAMBLE ISSUE  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnish\\AppData\\Local\\Temp\\ipykernel_14288\\4267916424.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('output_final 1 - 150000.csv')\n",
      "C:\\Users\\tnish\\AppData\\Local\\Temp\\ipykernel_14288\\4267916424.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  index = df.index[df['url'].str.contains(final_url_no_protocol)].tolist()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tnish\\Downloads\\MINI PROJECT\\DataPreProcessing.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# If not, remove the protocol from the final URL and search for the remaining URL in the URL column\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     final_url_no_protocol \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39mfinal_url\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m//\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     index \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex[df[\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mcontains(final_url_no_protocol)]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Check if the search result contains any elements before accessing the first element\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(index) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnish/Downloads/MINI%20PROJECT/DataPreProcessing.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39m# Concatenate the protocol with the final URL to get the correct final URL\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    125\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot use .str.\u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m with values of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minferred dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 129\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\accessor.py:1260\u001b[0m, in \u001b[0;36mStringMethods.contains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m regex \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mcompile(pat)\u001b[39m.\u001b[39mgroups:\n\u001b[0;32m   1253\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1254\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis pattern is interpreted as a regular expression, and has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmatch groups. To actually get the groups, use str.extract.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1256\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1257\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49m_str_contains(pat, case, flags, na, regex)\n\u001b[0;32m   1261\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_result(result, fill_value\u001b[39m=\u001b[39mna, returns_string\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\object_array.py:131\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_contains\u001b[1;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[0;32m    129\u001b[0m         upper_pat \u001b[39m=\u001b[39m pat\u001b[39m.\u001b[39mupper()\n\u001b[0;32m    130\u001b[0m         f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: upper_pat \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mupper()\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_str_map(f, na, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mdtype(\u001b[39m\"\u001b[39;49m\u001b[39mbool\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\object_array.py:71\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     69\u001b[0m map_convert \u001b[39m=\u001b[39m convert \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(mask)\n\u001b[0;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer_mask(arr, f, mask\u001b[39m.\u001b[39;49mview(np\u001b[39m.\u001b[39;49muint8), map_convert)\n\u001b[0;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     73\u001b[0m     \u001b[39m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     p_err \u001b[39m=\u001b[39m (\n\u001b[0;32m     76\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m((takes)|(missing)) (?(2)from \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ to )?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?(3)required )positional arguments?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\strings\\object_array.py:124\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_contains.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    120\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mIGNORECASE\n\u001b[0;32m    122\u001b[0m     pat \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(pat, flags\u001b[39m=\u001b[39mflags)\n\u001b[1;32m--> 124\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: pat\u001b[39m.\u001b[39;49msearch(x) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m case:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the CSV file\n",
    "df = pd.read_csv('output_final 1 - 150000.csv')\n",
    "\n",
    "# Create a new column for the corrected final URL\n",
    "df['corrected_final_url'] = ''\n",
    "\n",
    "# Keep track of the URLs that have already been mapped\n",
    "mapped_urls = []\n",
    "\n",
    "# Loop through the first 150000 rows\n",
    "for i in range(150000):\n",
    "    # Check if the URL from the final URL column is present in the URL column\n",
    "    if df.loc[i, 'final_url'] in mapped_urls:\n",
    "        # If it is, skip this iteration\n",
    "        continue\n",
    "    \n",
    "    index = df.index[df['url'] == df.loc[i, 'final_url']].tolist()\n",
    "    \n",
    "    # If the final URL is present in the URL column, map it to the corrected final URL column\n",
    "    if len(index) > 0:\n",
    "        df.at[i, 'corrected_final_url'] = df.loc[i, 'final_url']\n",
    "        mapped_urls.append(df.loc[i, 'final_url'])\n",
    "    else:\n",
    "        # If not, remove the protocol from the final URL and search for the remaining URL in the URL column\n",
    "        final_url_no_protocol = df.loc[i, 'final_url'].split('//')[-1]\n",
    "        index = df.index[df['url'].str.contains(final_url_no_protocol)].tolist()\n",
    "        \n",
    "        # Check if the search result contains any elements before accessing the first element\n",
    "        if len(index) > 0:\n",
    "            # Concatenate the protocol with the final URL to get the correct final URL\n",
    "            protocol = df.at[index[0], 'final_url'][:7]\n",
    "            corrected_final_url = protocol + final_url_no_protocol\n",
    "            df.at[i, 'corrected_final_url'] = corrected_final_url\n",
    "            mapped_urls.append(final_url_no_protocol)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69419369286f90106c380bbc1c47a0d17229dd8c0ff3d05cb5fd93397e6f9769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
